---
title: "Hierarchical Linear Regression Model building using RStan"
date: 2018-07-14
tags: [Bayes statistics]
header:
  image: "/images/whitepencil.jpg"
excerpt: "Bayes statistics, Data Science"
mathjax: "true"
---
**Introduction**

Inspired by the efficient building of the hierarchical model introduced in the textbook Bayesian Data Analysis, our group grew more interest in the flexible usage of RStan. We realized how cumbersome it can be in real-world application if we know the mathematical theory behind the model without a working knowledge in the software. As a result, we decided to explore more on variations in the application of the RStan package for Bayesian models. In addition, we’d like to take advantage of this opportunity to push ourselves one step towards the limits to fortify our understanding of Bayesian statistics. Specifically, we intend to combine the self-motivated study of Rstan with our current knowledge such as linear regression and simulation, which is also what we have been chewing on so-far this semester. From these perspectives, the hierarchical linear regression model is by all means a best choice to focus our attention on.

As for application of this model in real life problem, we decide to study fuel consumption for cars. Nowadays, petroleum consumption of cars is an important question that we pay attention to. In this case, we decide to study the “mtcars” dataset in R that contains some information about fuel consumption and its affected factors. Particularly, we are interested how gross horse power and rear axle ratio affect the miles per gallon for cars.

* Hierarchical Linear regression

While the normal linear regression captures only the dependency between the predictors and response, the hierarchical linear regression captures the further inner dependencies using nested groups, namely, hierarchy. These dependencies usually account for the relation among groups and between groups just as the hierarchical model we learned in class. As an easy example, if the original group is student, the higher-order group could be classroom, or even higher: region.

Usually, the hierarchical linear regression contains a 2-level structure, which is also we adopt in this project. Further levels could be considered but not very practical. The inclusion of higher than 2 levels in the hierarchical linear regression is similar to that of making assumptions on prior distributions of the hyper-parameters in the Bayesian context. In the 2-level case, we specify the individual-specific regression coefficients with a joint distribution at level 2. A comparison among different models: no pooling(separate regression without grouping), complete pooling(putting all in one group) and hierarchical(idea of partial pooling, sharing the advantage of both) shows that inappropriate grouping could yield completely opposite result from the truth.


* Application using RStan

To implement the theoretical ideas using programming language, RStan provides an efficiently way. As firstly learned from the 8 school hierarchical model demonstration, we outlined the routine program blocks in the “.stan” file as a specified model including all the assumed distributions, supplemented with data(the known values and their respective dimensions) along with parameters (the unknown values involved in the model). In our specific case, a block of transformed parameters is also introduced to make the expressions clearer. On the other hand, the known quantities such as the data as well as the estimates to be directly obtained from them are specified separately in the R environment.

This classification of different quantities into blocks in effect strengthened our understanding of the structure of the model in a comprehensive way. The details of our use of RStan to build the model will be unfolded in the following sections explaining our case of regression model.

**Mathematical Background**

The illustration of the hierarchical regression model starts with a normal linear regression:
$$Y = \beta_0 + X\beta + r$$, where X is the matrix design of the predictors.

Another convenient way(used in stan.) to represent linear regression is to write the response as a normal random variable, with all the assumptions in the regression coefficients and noise being normal, which is exactly the compact representation in RStan:

$$Y ~ Normal(\beta_0 + X\beta, \sigma^2), \mu= \beta_0 + X\beta$$

In our case, we extend the regression model to have a nested structure to make it a hierarchical linear regression model. That is, we divide the predictors further into several groups and for each group, the coefficients are regressed on (affected by) another set of predictors so that we vary the coefficients by grouping variables:

$$Y = \beta_0 + X\beta + r$$
Y = dependent variable
$$\beta_0 = intercept for level-2 unit$$
$$\beta = regression coefficient vector associated with X for level-2 unit$$
r = random error vector associated with level-1 unit nested within the level-2 unit
E(r) = 0; $$var(r) = sigma^2$$
Again, in normal distribution representation, the above can be written as:

$$Y ~ Normal (\beta_0 + X\beta, \sigma^2)$$

$$\beta ~ Normal(\gamma, \tau)$$
